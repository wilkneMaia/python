{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Data Science na Prática**\n",
    "*by [Wilkne Maia](https://wilkne)*\n",
    "\n",
    "---\n",
    "\n",
    "<h1>Parquet: O Formato que Está Transformando a Análise de Dados</h1>\n",
    "\n",
    "No universo da análise de dados, a eficiência e a velocidade são cruciais. É aqui que o formato Parquet entra em cena, revolucionando a forma como armazenamos e processamos informações. Mas o que torna o Parquet tão especial?\n",
    "\n",
    "O Parquet é um formato de armazenamento de dados colunar, open-source, que se destaca por otimizar a leitura e o processamento de grandes volumes de dados. Em vez de armazenar os dados linha a linha, como em formatos tradicionais (CSV, por exemplo), o Parquet organiza as informações por colunas. Essa diferença, aparentemente sutil, gera um impacto enorme na performance de consultas e análises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Por que o Parquet é tão relevante?**\n",
    "* **Eficiência de Leitura:** Ao armazenar dados por coluna, o Parquet permite que o sistema leia apenas as colunas necessárias para a análise, reduzindo drasticamente o tempo de I/O (entrada/saída) e o uso de recursos.\n",
    "* **Compressão Otimizada:** O formato é altamente compressível, o que significa arquivos menores e mais rápidos de transferir e armazenar, além de menor consumo de espaço em disco.\n",
    "* **Compatibilidade:** O Parquet é amplamente suportado por diversas ferramentas e plataformas de processamento de dados, como Spark, Hadoop, AWS Athena, Google BigQuery e muitas outras.\n",
    "* **Suporte a Schema:** Ele armazena o esquema dos dados, facilitando a leitura e interpretação das informações.\n",
    "\n",
    "**Em resumo:** O Parquet é um aliado poderoso para profissionais que lidam com grandes volumes de dados, seja em análise exploratória, data science, machine learning ou outras áreas. Sua capacidade de otimizar o processamento e o armazenamento o torna uma peça-chave no ecossistema de dados moderno."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar os Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instalar Parquet e Pandas Profiling\n",
    "%!pip install parquet\n",
    "%!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importar os pacotes necessários\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importar os dados do arquivo csv para um DataFrame no Pandas\n",
    "df = pd.read_csv('dados/PFW_2021_public.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar os Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importar os dados do DataFrame e salva em um arquivo Parquet\n",
    "df.to_parquet('dados/PFW_2021_public.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet = pd.read_parquet('dados/PFW_2021_public.parquet')\n",
    "df_parquet.shape\n",
    "df_parquet.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/2104080/how-do-i-check-file-size-in-python\n",
    "def convert_bytes(num):\n",
    "    \"\"\"\n",
    "    this function will convert bytes to MB.... GB... etc\n",
    "    \"\"\"\n",
    "    for x in ['bytes', 'KB', 'MB', 'GB', 'TB']:\n",
    "        if num < 1024.0:\n",
    "            return \"%3.1f %s\" % (num, x)\n",
    "        num /= 1024.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_em_bytes = os.path.getsize('PFW_2021_public.csv')\n",
    "convert_bytes(csv_em_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df = pd.read_csv('PFW_2021_public.csv')\n",
    "df.shape\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('PFW_2021_public.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_em_bytes = os.path.getsize('PFW_2021_public.parquet')\n",
    "convert_bytes(parquet_em_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_pfw_2021_parquet = pd.read_parquet('PFW_2021_public.parquet')\n",
    "df_pfw_2021_parquet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_pfw_2021_parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pfw_2021.to_parquet('PFW_2021_public.parquet.gzip', compression='gzip')\n",
    "\n",
    "parquet_gzip_em_bytes = os.path.getsize('PFW_2021_public.parquet.gzip')\n",
    "convert_bytes(parquet_gzip_em_bytes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
