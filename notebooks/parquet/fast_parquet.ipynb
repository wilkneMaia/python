{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import fastparquet as fp\n",
    "\n",
    "# Create a sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'Age': [25, 30, 35],\n",
    "    'Salary': [50000, 60000, 70000]\n",
    "})\n",
    "\n",
    "# Write the Arrow Table to a Parquet file\n",
    "fp.write('files/sample_fp.parquet', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Parquet file into a Pandas DataFrame\n",
    "df = fp.ParquetFile('files/sample_fp.parquet').to_pandas()\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write partitioned Parquet files\n",
    "fp.write('files/sample_fp.parquet', df, partition_on=['Age'])\n",
    "\n",
    "# Read a partitioned dataset\n",
    "df = fp.ParquetFile('files/sample_fp.parquet').to_pandas()\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Crie um DataFrame de amostra\n",
    "df = pd.DataFrame({\n",
    "    'Name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'Age': [25, 30, 35],\n",
    "    'Salary': [50000, 60000, 70000]\n",
    "})\n",
    "\n",
    "# Converta o DataFrame em uma Arrow Table\n",
    "table = pa.Table.from_pandas(df)\n",
    "\n",
    "# Escreva a Arrow Table em um arquivo Parquet\n",
    "pq.write_table(table, 'sample.parquet')\n",
    "\n",
    "# Leia o arquivo Parquet em uma Arrow Table\n",
    "table = pq.read_table('sample.parquet')\n",
    "\n",
    "# Converta a Arrow Table em um Pandas DataFrame\n",
    "df = table.to_pandas()\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write partitioned Parquet files\n",
    "pq.write_to_dataset(table, root_path='dataset/', partition_cols=['Age'])\n",
    "\n",
    "# Read a partitioned dataset\n",
    "table = pq.ParquetDataset('dataset/').read()\n",
    "df = table.to_pandas()\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Name  Age  Salary\n",
      "0    Alice   25   50000\n",
      "1      Bob   30   60000\n",
      "2  Charlie   35   70000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Crie um DataFrame de amostra\n",
    "df = pd.DataFrame({\n",
    "    'Name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'Age': [25, 30, 35],\n",
    "    'Salary': [50000, 60000, 70000]\n",
    "})\n",
    "\n",
    "# Converta o DataFrame em uma Arrow Table\n",
    "table = pa.Table.from_pandas(df)\n",
    "\n",
    "# Escreva a Arrow Table em um arquivo Parquet\n",
    "pq.write_table(table, 'dataset/sample.parquet')\n",
    "\n",
    "# Leia o arquivo Parquet em uma Arrow Table\n",
    "table = pq.read_table('dataset/sample.parquet')\n",
    "\n",
    "# Converta a Arrow Table em um Pandas DataFrame\n",
    "df = table.to_pandas()\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Name  Age  Salary\n",
      "0    Alice   25   50000\n",
      "1      Bob   30   60000\n",
      "2  Charlie   35   70000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Lendo um arquivo Parquet\n",
    "try:\n",
    "    df = pd.read_parquet('dataset/sample.parquet')\n",
    "    print(df.head())\n",
    "except FileNotFoundError:\n",
    "    print(\"Arquivo 'dataset/sample.parquet' não encontrado. Certifique-se de que o arquivo existe no diretório atual.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro ao ler o arquivo Parquet: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Original:\n",
      "      Nome  Idade   Salário Data_Admissão  \\\n",
      "0    Maria   25.0  50000.50    2020-01-15   \n",
      "1     John   30.0  60000.75    2019-05-20   \n",
      "2   Tonico   35.0  70000.00    2018-11-01   \n",
      "3  Mariane    NaN  80000.25    2021-03-10   \n",
      "\n",
      "                                Descrição  \n",
      "0                   Desenvolvedora Python  \n",
      "1                       Analista de Dados  \n",
      "2                      Cientista de Dados  \n",
      "3  Gerente de Projetos com acentuação çãõ  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = {\n",
    "    'Nome': ['Maria', 'John', 'Tonico', 'Mariane'],\n",
    "    'Idade': [25, 30, 35, np.nan],  # Incluindo um valor nulo\n",
    "    'Salário': [50000.50, 60000.75, 70000.00, 80000.25],\n",
    "    'Data_Admissão': pd.to_datetime(['2020-01-15', '2019-05-20', '2018-11-01', '2021-03-10']),\n",
    "    'Descrição': ['Desenvolvedora Python', 'Analista de Dados', 'Cientista de Dados', 'Gerente de Projetos com acentuação çãõ']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"DataFrame Original:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Arquivo Parquet 'meu_arquivo.parquet' escrito com sucesso usando pyarrow e compressão snappy!\n",
      "\n",
      "Arquivo Parquet 'meu_arquivo_fastparquet.parquet' escrito com sucesso usando fastparquet!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df.to_parquet('meu_arquivo.parquet', engine='pyarrow', compression='snappy')\n",
    "    print(\"\\nArquivo Parquet 'meu_arquivo.parquet' escrito com sucesso usando pyarrow e compressão snappy!\")\n",
    "\n",
    "    df.to_parquet('meu_arquivo_fastparquet.parquet', engine='fastparquet')\n",
    "    print(\"\\nArquivo Parquet 'meu_arquivo_fastparquet.parquet' escrito com sucesso usando fastparquet!\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao escrever o arquivo Parquet: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame lido completo com pyarrow:\n",
      "      Nome  Idade   Salário Data_Admissão  \\\n",
      "0    Maria   25.0  50000.50    2020-01-15   \n",
      "1     John   30.0  60000.75    2019-05-20   \n",
      "2   Tonico   35.0  70000.00    2018-11-01   \n",
      "3  Mariane    NaN  80000.25    2021-03-10   \n",
      "\n",
      "                                Descrição  \n",
      "0                   Desenvolvedora Python  \n",
      "1                       Analista de Dados  \n",
      "2                      Cientista de Dados  \n",
      "3  Gerente de Projetos com acentuação çãõ  \n",
      "\n",
      "DataFrame lido completo com fastparquet:\n",
      "      Nome  Idade   Salário Data_Admissão  \\\n",
      "0    Maria   25.0  50000.50    2020-01-15   \n",
      "1     John   30.0  60000.75    2019-05-20   \n",
      "2   Tonico   35.0  70000.00    2018-11-01   \n",
      "3  Mariane    NaN  80000.25    2021-03-10   \n",
      "\n",
      "                                Descrição  \n",
      "0                   Desenvolvedora Python  \n",
      "1                       Analista de Dados  \n",
      "2                      Cientista de Dados  \n",
      "3  Gerente de Projetos com acentuação çãõ  \n",
      "\n",
      "DataFrame com colunas selecionadas (Nome e Salário) com pyarrow:\n",
      "      Nome   Salário\n",
      "0    Maria  50000.50\n",
      "1     John  60000.75\n",
      "2   Tonico  70000.00\n",
      "3  Mariane  80000.25\n"
     ]
    }
   ],
   "source": [
    "# Lendo o arquivo Parquet\n",
    "try:\n",
    "    # Lendo o arquivo completo com pyarrow (recomendado)\n",
    "    df_lido_pyarrow = pd.read_parquet('meu_arquivo.parquet', engine='pyarrow')\n",
    "    print(\"\\nDataFrame lido completo com pyarrow:\")\n",
    "    print(df_lido_pyarrow)\n",
    "\n",
    "    # Lendo o arquivo completo com fastparquet\n",
    "    df_lido_fastparquet = pd.read_parquet('meu_arquivo_fastparquet.parquet', engine='fastparquet')\n",
    "    print(\"\\nDataFrame lido completo com fastparquet:\")\n",
    "    print(df_lido_fastparquet)\n",
    "\n",
    "    # Lendo apenas as colunas 'Nome' e 'Salário' com pyarrow (column pruning)\n",
    "    df_colunas_selecionadas = pd.read_parquet('meu_arquivo.parquet', columns=['Nome', 'Salário'], engine='pyarrow')\n",
    "    print(\"\\nDataFrame com colunas selecionadas (Nome e Salário) com pyarrow:\")\n",
    "    print(df_colunas_selecionadas)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"\\nErro: Arquivo Parquet não encontrado. Certifique-se de que o arquivo 'meu_arquivo.parquet' ou 'meu_arquivo_fastparquet.parquet' existe no diretório atual.\")\n",
    "except pd.errors.ParserError as pe:\n",
    "    print(f\"\\nErro de parsing do Parquet: {pe}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nOutro erro ao ler o arquivo Parquet: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame lido completo com pyarrow:\n",
      "      Nome  Idade   Salário Data_Admissão  \\\n",
      "0    Maria   25.0  50000.50    2020-01-15   \n",
      "1     John   30.0  60000.75    2019-05-20   \n",
      "2   Tonico   35.0  70000.00    2018-11-01   \n",
      "3  Mariane    NaN  80000.25    2021-03-10   \n",
      "\n",
      "                                Descrição  \n",
      "0                   Desenvolvedora Python  \n",
      "1                       Analista de Dados  \n",
      "2                      Cientista de Dados  \n",
      "3  Gerente de Projetos com acentuação çãõ  \n",
      "\n",
      "DataFrame lido completo com fastparquet:\n",
      "      Nome  Idade   Salário Data_Admissão  \\\n",
      "0    Maria   25.0  50000.50    2020-01-15   \n",
      "1     John   30.0  60000.75    2019-05-20   \n",
      "2   Tonico   35.0  70000.00    2018-11-01   \n",
      "3  Mariane    NaN  80000.25    2021-03-10   \n",
      "\n",
      "                                Descrição  \n",
      "0                   Desenvolvedora Python  \n",
      "1                       Analista de Dados  \n",
      "2                      Cientista de Dados  \n",
      "3  Gerente de Projetos com acentuação çãõ  \n",
      "\n",
      "DataFrame com colunas selecionadas (Nome e Salário) com pyarrow:\n",
      "      Nome   Salário\n",
      "0    Maria  50000.50\n",
      "1     John  60000.75\n",
      "2   Tonico  70000.00\n",
      "3  Mariane  80000.25\n"
     ]
    }
   ],
   "source": [
    "# Lendo o arquivo Parquet\n",
    "try:\n",
    "    # Lendo o arquivo completo com pyarrow (recomendado)\n",
    "    df_lido_pyarrow = pd.read_parquet('meu_arquivo.parquet', engine='pyarrow')\n",
    "    print(\"\\nDataFrame lido completo com pyarrow:\")\n",
    "    print(df_lido_pyarrow)\n",
    "\n",
    "    # Lendo o arquivo completo com fastparquet\n",
    "    df_lido_fastparquet = pd.read_parquet('meu_arquivo_fastparquet.parquet', engine='fastparquet')\n",
    "    print(\"\\nDataFrame lido completo com fastparquet:\")\n",
    "    print(df_lido_fastparquet)\n",
    "\n",
    "    # Lendo apenas as colunas 'Nome' e 'Salário' com pyarrow (column pruning)\n",
    "    df_colunas_selecionadas = pd.read_parquet('meu_arquivo.parquet', columns=['Nome', 'Salário'], engine='pyarrow')\n",
    "    print(\"\\nDataFrame com colunas selecionadas (Nome e Salário) com pyarrow:\")\n",
    "    print(df_colunas_selecionadas)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"\\nErro: Arquivo Parquet não encontrado. Certifique-se de que o arquivo 'meu_arquivo.parquet' ou 'meu_arquivo_fastparquet.parquet' existe no diretório atual.\")\n",
    "except pd.errors.ParserError as pe:\n",
    "    print(f\"\\nErro de parsing do Parquet: {pe}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nOutro erro ao ler o arquivo Parquet: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
